---
title: 'Advanced Statistical Modelling: Linear Models'
author: "Joel Cantero Priego and Ricard Meyerhofer Parra"
date: "12/10/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library("kableExtra")

```

## Introduction

In this assignment, we are going to use the IMDB dataset. This IMDB dataset, contains information of 940 films released between 2000 and 2016. The data has been obtained from the IMDB's webpage. The following is a list where we can see all the variables of the dataset:

Variable name         | Description                          | Values
:--------------:|:-------------------------------------:|:-------------------------------------:
movietitle| Director of the given title | String |
gross| Gross in dollars |	Integer |
budget|Budget in dollars	| Integer |
duration| Film duration in minutes | Integer |
titleyear| The release year of the title	| Integer |
directorfl| Director Facebook likes	| Integer |
actor1fl| Actor 1 Facebook likes | Integer |
actor2fl| Actor 2 Facebook likes | Integer |
actor3fl| Actor 3 Facebook likes | Integer |
castfl| Cast Facebook likes | Integer |
facenumber_in_poster| Number of faces that appears in the poster | Integer |
genre | Genre film | Action/Comedy/Drama/Terror

As we can see we have that all our variables are numerical in exception genre. This dataset is complete which means that it has no missing values. However, this does not imply that there are no outliers. 
Because this dataset is a simplification from an IMDB dataset we are not going to perform inputations on we might think are outliers. Furthermore, we have to take into consideration that this dataset has a very wide variety of movies so the spectrum of values can be really different. 

```{r read-data, include=FALSE}
dataset=read.csv2("IMDB.csv")
head(dataset)
summary(dataset)
```

```{r, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
library(mice)
md.pattern(dataset)
```

As required in the assignment, we are going to create a categorical variable: **yearcat** which is the categorical substitution of titleyear with 3 levels: 2000-2005, 2006-2010 and 2011-2016. Therefore, we will have two categorical variables (genre and titleyear).  

```{r titleyear-to-categorical, echo=TRUE}
dataset$yearcat<-as.factor(cut(dataset$titleyear, 
                     c(2000,2005,2010,2016), 
                     include.lowest = TRUE, 
                     labels=c("2000-2005", "2006-2010", "2011-2016")))
```



## Exploratory Data Analysis
In this section we are going to focus in explaining the most interesting conclusions of our data, perform an univariate and multivariate analysis of the variables in order to find outliers and see how each of these variables is related with the gross. We are also going to modify some variables in order to make the linear model perform better on them.


###Movie title

We have performed a cloud of the most relevant words that appear in the movies. To do so, we have removed stopwords and punctuation. We could have also done a stemming process but is not so important for us to do so.
We can see that the top words are words such as: man, love, movie, house, american, life, big, etc.

```{r, echo=FALSE, fig.height=4, fig.width=12, message=FALSE, warning=FALSE}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")

text <- dataset$movietitle
# Load the data as a corpus
docs <- Corpus(VectorSource(text))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
#docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word

# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
# docs <- tm_map(docs, stemDocument)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
#words that occur at least 4 times
#findFreqTerms(dtm, lowfreq = 4)
#terms that associate with word
#findAssocs(dtm, terms = "man", corlimit = 0.3)
```

###Budget

We can see that there is a very disperse amount of values regarding the budget that range from a minimum of 400 thousand dollars (Napoleon Dynamite) to 300 million dollars (Pirates of the Caribbean: At World's End). Despite how crazy this numbers can appear to be, we have revised them by looking at the budget of this two movies on the internet and are correct. 
Note that this does not imply that all the budgets we have are corrects but it implies that we have to deal with such a range of different values in a same variable.

```{r, message=FALSE, warning=FALSE, include=FALSE}
dataset$movietitle[dataset$budget==400000]
dataset$movietitle[dataset$budget==300000000]
```

```{r, echo=FALSE, fig.height=4, fig.width=12}
par(mfrow=c(1,2))
hist(dataset$budget, main="Histogram of budget")
boxplot(dataset$budget, main="Box Plot of budget")
```

####Relationship between budget and gross

We can see that the revenue is correlated with the budget as we can see in the plot below.
```{r, echo=FALSE, fig.height=4, fig.width=12}
plot(dataset$budget, dataset$gross)
```

###Gross

If we take a look at the gross variable, we can see that in a similar fashion than with budget, we have a range of values that can go from 3330$ with Mi America to Avatar with 760 milions of dollars.

```{r, echo=TRUE, fig.height=4, fig.width=12}
par(mfrow=c(1,3))
boxplot(dataset$gross)
plot(density(dataset$gross), main="Density plot of gross")
hist(dataset$gross)
```


As we have just seen values from budget and gross are in a bigger scale than the rest of our data. This is a problem when performing a linear model since it adds complexity to the model. In order to avoid so, we are going to scale those variables. We decided to apply $\sf{log_{10}}$ because it is easier to interprate later when showing (insert justification).


```{r dollars, eval=FALSE, fig.height=4, fig.width=12, include=FALSE}
dataset$gross <- log10(dataset$gross)
dataset$budget <- log10(dataset$budget)
dataset$duration <- dataset$duration
#dataset$directorfl <- log10(dataset$directorfl) cannot bc of 0s
#dataset$actor1fl <- log10(dataset$actor1fl)
#dataset$actor2fl <- log10(dataset$actor2fl)
#dataset$actor3fl <- log10(dataset$actor3fl)
#dataset$castfl <- log10(dataset$castfl)
par(mfrow=c(1,2))
plot(density(dataset$gross), main="Density plot of gross")
plot(density(dataset$budget), main="Density plot of budget")
```

Now we can see that even it does not follow a normal distribution completely it starts to look like one and what is more important is that the range of values is smaller for both, budget and gross.


###Duration

In duration film, we see that there is a certain tendency to normality centered around 100 minutes, we consider it as usual. There is a strange observation of 280 minutes for "Gods and General" film. After we check it, we can say that it is not an error but an extreme value. 

```{r, echo=FALSE, fig.height=4, fig.width=12}
par(mfrow=c(1,2))
hist(dataset$duration, main="Histogram of film duration")
boxplot(dataset$duration, main="Box Plot of film duration")
```


####Relationship between duration and gross

We can see that there is not a clear correlation between the duration of a film and its revenue since we can see that transversally to the duration, we have the similar results in gross. It is true that the tendence over 150 minutes might be different but is not significant enough to take any conclusion from it.
```{r, echo=FALSE, fig.height=4, fig.width=12}
plot(dataset$duration, dataset$gross)
```

###Title Year

No problems for year, there is a certain expected balanced in years proportion even that we can see a decay in the number of films for 2016.

```{r, echo=FALSE, fig.height=4, fig.width=12}
barplot(table(dataset$titleyear), main="Bar Chart of title year")
```

####Relationship between years and gross
We can see that there is not a strong correlation between the year of release and the gross obtained from those years. Even that there are some years better than others the trend seems to be the same across the years.

```{r, echo=FALSE, fig.height=4, fig.width=12}
plot(dataset$titleyear, dataset$gross)
```

###Directorfl, Actor1fl, Actor2fl, Actor3fl, Castfl

In Director Facebook likes, we see that there is a value which appears in the majority of the cases: In this case, is the 0 value. Apart from this zero value, we see that small number of likes are more common than medium or higher number of likes. 

```{r, echo=FALSE, fig.height=4, fig.width=12}
plot(density(dataset$directorfl), main="Density plot of Director Facebook Likes")
#length(which(dataset$directorfl==0))/length(dataset$directorfl)
```
We can see that the other variables Actor1fl, Actor2fl, Actor3fl, Castfl follow a similar fashion.


####Relationship of Directorfl, Actor1fl, Actor2fl, Actor3fl, Castfl with gross

We can see a trend where the more likes Directors, Acts or Cast have are not correlated with the gross of the movie we even see cases where there is an extreme value of likes which does not project in a high revenue.

```{r, echo=FALSE, fig.height=4, fig.width=12}
par(mfrow=c(2,3))
plot(dataset$directorfl, dataset$gross)
plot(dataset$actor1fl, dataset$gross)
plot(dataset$actor2fl, dataset$gross)
plot(dataset$actor3fl, dataset$gross)
plot(dataset$castfl, dataset$gross)
```

###Facenumber in poster

In face number in film poster, the mean is about 1,6 faces and we can observe an extrem value of 31 in "The Master". We can say once again that it is not an error but an extreme value.

```{r, echo=FALSE, fig.height=4, fig.width=12}
par(mfrow=c(1,3))
plot(dataset$facenumber_in_poster)
#no em molen gens les gràfiques que fas per general. No són gens indicatives ni són clares. el hist no té sentit
hist(dataset$facenumber_in_poster, main="Histogram of face number")
boxplot(dataset$facenumber_in_poster, main="Box Plot of face number")
```

####Relationship of Facenumber with Gross
We can see that the number of faces in a poster that appear seems not to be related with the revenue directly. However, we could even say that than 6-7 faces in poster does not seem to be a positive factor to have a good revenue.

```{r, echo=FALSE, fig.height=4, fig.width=12}
plot(dataset$facenumber_in_poster, dataset$gross)
```

###Genre
In genre film we can observe that there are more comedy and drama films than action and terror films. 


```{r, echo=FALSE, fig.height=4, fig.width=12}
par(mfrow=c(1,2))
barplot(table(dataset$genre), main="Bar Chart of film genre")
library("wordcloud")
set.seed(1234)
wordcloud(words = c("Action", "Comedy","Drama", "Terror"), freq = c(112, 365, 330, 133) , min.freq = 50,
          max.words=5, random.order=FALSE, rot.per=0.45, 
          colors=brewer.pal(3, "Dark2"))
```

####Relationship of Genre with Gross

We can see that action films from the dataset tend to have a slightly more income than drama, terror and comedy films. We also can see that there are more outliers in the comedy genre.

```{r, echo=FALSE, fig.height=4, fig.width=12}
plot(dataset$genre, dataset$gross)
```

## Correlation matrix
In this section we can see the correlation between the varibales. It is a bit of what we have seen but in this case is not only focused with the gross but all the variables. Furthermore, by doing the correlation of the variables, we have a numeric value that says us how correlated two variables are which we did not quantify when doing the exploratory analysis.

We can see that there is a positive correlation between gross and budget variable (0,729). On the other hand there is positive correlation between Cast Facebook Likes and Actor Facebook likes. 

```{r pressure, echo=FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=12}
pairs(~.,dataset[,-c(1)])
kable(cor(dataset[,-c(1, 5, 12, 13)]), format="latex", booktabs=TRUE) %>% 
  kable_styling(latex_options="scale_down")
```


## Fitting the complete model
In this section we are going to perform a linear model with all the variables and their combinations so that we can see how it performs. We are going to exclude movietitle because it is a string and titleyear because we are going to user the factor we created previously (yearcat).

```{r, echo=TRUE, fig.height=4, fig.width=12, message=FALSE, warning=FALSE, results = 'hide'}
#gross ~ (. - titleyear - movietitle) * (yearcat + genre),

summary(completeModel<-lm(gross ~ (budget + duration + directorfl + actor1fl + actor2fl 
                                   + actor3fl + castfl + facenumber_in_poster 
                                   + genre + yearcat)^2, dataset))

```

As we can see in the NormQ-Q our model does not adjust perfectly to the model since we are having an adjusted r-squared of 0.50 and we are performing this model with all the possible combination of variables, which is a huge model. This is not necessary and should be avoided under all circumstances since our aim should be to have the minimal model possible which in the end, will be the one that will generalise the better too.

```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.height=6, fig.width=12}
op<-par(mfrow=c(2,2))
plot(completeModel)
```

## Use the stepwise procedure, by using the BIC criterion, to select the signicant variables

In order to create a model with the most significant variables, we decided to choose the BIC criterion. We could choose BIC over AIC because BIC is more restrictive therefore we will have less variables which is what we are aiming for. In order to use the standart weight which is 2, we decided to use the log of the size of variables which will be more restrictive and will remove those interactions that are not relevant.

We decided to compare two heuristic strategies when modelling: 

- **Forward:** In this strategy we start the case with none available predictor variables and add one at a time

- **Backward:** In this strategy we start with all available predictor variables and delete one at a
time

In any case we are later going to validate that the variables obtained from each model are significant.

```{r, echo=TRUE, fig.height=4, fig.width=12, message=FALSE, warning=FALSE, results = 'hide'}
nullModel <- lm(gross ~ 1, dataset)

forwardModel <- step(nullModel, 
                     scope = list(upper=completeModel), 
                     direction="both", criterion = "BIC", 
                     k=log(nrow(dataset)))

backwardModel <- step(completeModel, 
                      scope = list(lower=nullModel), 
                      direction="both", 
                      criterion = "BIC", 
                      k=log(nrow(dataset)))

``` 

We can see that the forward model is a simpler model that hass less variables and all of them pass the p-value test.
```{r, echo=TRUE, fig.height=3, fig.width=12, message=FALSE, warning=FALSE,fig.align='center'}
kable(summary(forwardModel)$coefficients, format="latex", booktabs=TRUE)  %>%
  kable_styling(position = "center")
```

In contraposition with the forward model, here we have a way more complex model and not all their variables are significant. 
```{r, echo=TRUE, fig.height=3, fig.width=12, message=FALSE, warning=FALSE,fig.align='center'}
kable(summary(backwardModel)$coefficients, format="latex", booktabs=TRUE) %>%
  kable_styling(position = "center")
```

Both have an R-squared of around 0.5 so their performance is similar. But as we said we are mostly interested in simple models so the forward model looks better to us.

## Check the presence of multicollinearity. If there is some non-interaction multicollinearity in the model, make the corresponding corrections.
We are now going to evaluate the multicollinearity of the models, which states if one prediction variable can be linearly predicted with the others in a substantial degree of accuracy. To evaluate the multicollinearity we are going to compute the VIF.


```{r, echo=TRUE, fig.height=2, fig.width=12, message=FALSE, warning=FALSE, fig.align='center'}
kable(car::vif(forwardModel), format="latex", booktabs=TRUE)%>%
  kable_styling(position = "center")
kable(car::vif(backwardModel), format="latex", booktabs=TRUE)%>%
  kable_styling(position = "center")
```

By checking the values of VIF, we see that from the forward model that is the simplest, the only variables that are significant, are budget and duration. 
```{r, echo=TRUE, fig.height=4, fig.width=12, message=FALSE, warning=FALSE}
finalModel <- lm(gross ~ budget + duration, dataset)
kable(summary(finalModel)$coefficients, format="latex", booktabs=TRUE)%>%
  kable_styling(position = "center")
```


```{r, echo=TRUE, fig.height=4, fig.width=12, message=FALSE, warning=FALSE}
finalModel2 <- lm(gross ~ duration + actor3fl + genre 
                  + yearcat + budget:genre + budget:yearcat, dataset)
kable(summary(finalModel2)$coefficients, format="latex", booktabs=TRUE)%>%
  kable_styling(position = "center")
```
In this model we have that the variable yearcat2006-2010 does have multicolinearity together with  yearcat2006-2010:budget. However, if we revise both models we see that this second model is way more complex and even that it performs slightly better, it is more complex. Therefore, we are going to choose **finalModel** which is the one of the **forwardModel**

## Validate the model by checking the assumptions
Once we have our model selected, we are going to validate it with anova comparing the null model with our final model.
```{r, echo=TRUE, fig.height=2, fig.width=12, message=FALSE, warning=FALSE}
anova(nullModel, finalModel)
```
In this case we can see that our model, **passes** the test since our p-value is lower than 0.05. 

## Interpret the final model

Even our final model has an adjusted r-squared of 0.4928 which is a low value, we can assume that does not exist multicollinearity in its variables, one of our purposes in this project.

As we have previously said, we would rather a simple model with two predictor variables than a complex model with many of them. It seems difficult to predict gross variable in this dataset due its size (just 940 observations) and the very different data that contains. Probably it would be much wiser to select movies from a certain revenue and classify them and create a model for them. Regardless, we do not have this many variables that can be relevant as we have seen in the exploratory analysis.

We can say that other kind of models should lead to better results than a simple linear regression model. Also performing a PCA would give a first insight on how variables are related to each other.

Finally, we can see that our model now has a worst performance than the complete model but in this case we are not using all variables but rather a very simple model. 

We can see from the residuals vs fitted that we still have non-linear patterns but not very differently than in the aforementioned. In the same way, now we have more skewness in the tail of the Q-Q plot. We can see from the Scale-Location that from 1.75e+08 the residuals are wider.

```{r, echo=TRUE, fig.height=6, fig.width=12, message=FALSE, warning=FALSE}
op=par(mfrow=c(2,2))
plot(finalModel)
```


```{r, echo=TRUE, fig.height=6, fig.width=12, message=FALSE, warning=FALSE}
library(car)
scatterplot(predict(finalModel),dataset$gross,smooth=FALSE)
```