---
title: 'Advanced Statistical Modelling: Ridge Regression'
author: "Ricard Meyerhofer & Joel Cantero"
date: "29/10/2019"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("ggplot2")
library("car")
library("caret")
library("dplyr")
library("GGally")
library("kableExtra")
library("glmnet")
library("plotmo")
library("data.table")
set.seed(42)
```

# Choosing the penalization parameter $\lambda$
The objective of this exercise is to implement Ridge Regression with two different approaches: $MSPE_{val}(\lambda)$ and $MSPE_{k-CV}(\lambda)$. In both cases we are going to take as input data the following:
\begin{itemize}
\item Matrix x and vector y corresponding to the training sample.
\item Matrix $x_{val}$ and vector $y_{val}$ corresponding to the validation set.
\item Vector $lambda.v$ of candidate values for $\lambda$.
\end{itemize}
We are going to output for each element $\lambda$ in $lambda.v$ and the value of the $MSPE_{val}(\lambda)$ / $MSPE_{k-CV}(\lambda)$. Furthermore, we are going to plot these values against $log(1+ \lambda)-1$.

Once we have build these two functions, we are going to use the prostate data used in class. We are going to choose a $\lambda$ according to:
\begin{itemize}
\item Behaviour in the validation set (30 validations not included in the training sample)
\item 5-fold, 10-fold cross-validation.
\item Compare our results with those obtained with leave-one-out and generalized cross-validation.
\end{itemize}

## Ridge regression based on $MSPE_{val}(\lambda)$ 
In order to choose the penalization parameter $\lambda$, we are going to write a function implementing the ridge regression penalization parameter $\lambda$ choice based on the minimization of the $MSPE_{val}(\lambda)$.


```{r lambda_function}
ridge <- function(x, y, x.val, y.val, lambda.v) {
  result <- data.frame(lambda=lambda.v, mspe=rep(0,length(lambda.v)), 
                       df=rep(0,length(lambda.v)))
  x.svd <- svd(x)
  for(i in 1:length(lambda.v)) {
    lambda <- lambda.v[i]
    d_inv <- diag(1/(x.svd$d*x.svd$d - lambda))
    xx_inv <- t( solve( t(x) %*% x + lambda*diag(1,ncol(x)) ))
    beta <- xx_inv %*% t(x) %*% y
    y.hat <- x.val %*% beta
    mspe <- sum((y.val - y.hat)^2) / length(y.val)
    df <- sum(x.svd$d^2 / (x.svd$d^2 +lambda))
    result$mspe[i] <- mspe
    result$df[i] <- df
  }
  plot(mspe~log(1+lambda), result, col=2)
  lambda.min <- result$lambda[which.min(result$mspe)]
  abline(v=log(1+lambda.min),col=2,lty=2)
  plot(mspe~df, result, col=3)
  df.min <- result$df[which.min(result$mspe)]
  abline(v=df.min,col=3,lty=2)
  return(result)
}

```



## Ridge regression based on $MSPE_{k-CV}(\lambda)$
Now, we will write an R function implementing the ridge regression penalization parameter$\lambda$ choice based on k-fold cross-validation $MSPE_{kâˆ’CV}(\lambda)$

```{r}

ridge_cv <- function(x, y, lambda.v, cv=10) {
  result <- data.frame(lambda=lambda.v, mspe=rep(0,length(lambda.v)), 
                       df=rep(0,length(lambda.v)))
  for(i in 1:length(lambda.v)) {
    lambda.v[i] <- lambda.v[i]
    folds <- createFolds(1:nrow(x), k = cv)
    result.cv <- data.frame(mspe=rep(0,cv), df=rep(0,cv))
    for(j in 1:cv) {
      fold <- folds[[j]]
      x.train <- x[-fold,]
      y.train <- y[-fold]
      x.val <- x[fold,]
      y.val <- y[fold]
      x.svd <- svd(x.train)
      d <- x.svd$d
      v <- x.svd$v
      d_inv <- diag(1/(d*d - lambda.v[i]))
      xx_inv <- t( solve( t(x.train) %*% x.train + lambda.v[i]*diag(1,ncol(x)) ))
      beta <- xx_inv %*% t(x.train) %*% y.train
      y.hat <- x.val %*% beta
      mspe <- sum((y.val - y.hat)^2) / length(y.val)
      df <- sum(d^2 / (d^2 +lambda.v[i]))
      result.cv$mspe[j] <- mspe
      result.cv$df[j] <- df
    }
    result$mspe[i] <- mean(result.cv$mspe)
    result$df[i] <- mean(result.cv$df)
    
  }
  plot(mspe~log(1+lambda), result, col=2)
  lambda.min <- result$lambda[which.min(result$mspe)]
  abline(v=log(1+lambda.min),col=2,lty=2)
  plot(mspe~df, result, col=3)
  df.min <- result$df[which.min(result$mspe)]
  abline(v=df.min,col=3,lty=2)
  
  return(result)
  
}

```

## Comparation between penalization parameters of $MSPE_{val}(\lambda)$ and $MSPE_{k-CV}(\lambda)$

```{r prostate}
ridge_loocv_gcv <- function(x, y, lambda.v) {
  l <- length(lambda.v)
  result <- data.frame(lambda=lambda.v, loocv=rep(0,l), gcv=rep(0,l),df=rep(0,l))
  n <- nrow(x)
  x.svd <- svd(x)
  d <- x.svd$d
  v <- x.svd$v
  u <- x.svd$u
  for(i in 1:l) {
    lambda <- lambda.v[i]
    d_inv <- diag(1/(d^2 - lambda))
    xx_inv <- t( solve( t(x) %*% x + lambda*diag(1,ncol(x)) ))
    beta <- (xx_inv %*% t(x)) %*% y
    y.hat <- x %*% beta
    df <- sum(d^2 / (d^2 +lambda))
    h <- x %*% xx_inv %*% t(x)
    result$loocv[i] <- sum( ( (y - y.hat)/(1 - diag(h)) )^2 ) / n
    result$gcv[i] <- sum( ( (y - y.hat)/(1 - df/n) )^2 ) / n
    result$df[i] <- df
  }
  plot(loocv~log(1+lambda), result, col=2)
  lambda.min <- result$lambda[which.min(result$loocv)]
  abline(v=log(1+lambda.min),col=2,lty=2)
  plot(loocv~df, result, col=3)
  df.min <- result$df[which.min(result$loocv)]
  abline(v=df.min,col=3,lty=2)
  plot(gcv~log(1+lambda), result, col=2)
  lambda.min <- result$lambda[which.min(result$gcv)]
  abline(v=log(1+lambda.min),col=2,lty=2)
  plot(gcv~df, result, col=3)
  df.min <- result$df[which.min(result$gcv)]
  abline(v=df.min,col=3,lty=2)
  return(result)
}
```

```{r, echo=TRUE}

prostate <- fread(file="prostate_data.txt", header=TRUE)
prostate$train <- as.logical(prostate$train)
prostate$No <- NULL
prostate$train <- as.logical(prostate$train)
prostate <- as.data.frame(prostate)
data <- prostate[,1:9]
lambda.max <- 1e5
n.lambdas <- 25
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
validation.ind <- prostate$train
validation <- data[!validation.ind,]
training <- data[validation.ind,]

x <- scale(training[,1:8], center = T, scale = T)
y <- scale(training[,9], center = T, scale = F)

x.val <- scale(validation[,1:8], center = T, scale = T)
y.val <- scale(validation[,9], center = T, scale = F)
op<-par(mfrow=c(2,2))
result.valid <- ridge(x, y, x.val, y.val, lambda.v)
# 5-fold and 10-fold CV
# With CV 
x <- scale(data[,1:8], center = T, scale = T)
y <- scale(data[,9], center = T, scale = F)
op<-par(mfrow=c(2,2))
result.5.cv <- ridge_cv(x, y, cv = 10, lambda.v)
result.10.cv <- ridge_cv(x, y, cv = 10, lambda.v)
# LOOCV and GCV estimates
op<-par(mfrow=c(2,2))
result.loocv <- ridge_cv(x, y, cv = nrow(x), lambda.v)
result.loocv.gcv <- ridge_loocv_gcv(x, y, lambda.v)
```

We observe that the results of MSPE seem more stable in the LOOCV and GCV compared to 5-fold and 10-fold CV, as one would expect given the small size of the dataset. Furthermore, the validation set results are also relatively stable, although with a different curve than for the LOOCV/GCV.



# Ridge regression for the Boston Housing data

The Boston Housing dataset is a classical dataset which containes the values of 506 suburbs of Boston corresponding to 1978. This dataset can be found in many places but we are going to use a version with some corrections that was provided to us, which additionally includes the UTM coordinates of the geographical centers of each neighborhood. Therefore, the variables are the following:

Variable  | Description                           | Type
:--------------:|:-----------------------------------------------------------------------------------:|:--------------:
CRIM | per capita crime rate by town |  Numeric |
ZN | proportion of residential land zoned for lots over 25,000 sq.ft. |  Numeric|
INDUS | proportion of non-retail business acres per town |  Numeric|
CHAS | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) | Factor |
NOX | nitric oxides concentration (parts per 10 million) |  Numeric|
RM | average number of rooms per dwelling |  Numeric |
AGE | proportion of owner-occupied units built prior to 1940 | Numeric |
DIS | weighted distances to five Boston employment centres | Numeric |
RAD | index of accessibility to radial highways | Numeric |
TAX | full-value property-tax rate per $10,000 |  Numeric |
PTRATIO | pupil-teacher ratio by town |  Numeric |
B | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town | Numeric  |
LSTAT | % lower status of the population | Numeric  |
MEDV | Median value of owner-occupied homes in $1000's | Numeric |


In this exercise we are going to use ridge regression on the Boston Housing dataset to fit the regression model where the response variable is $MEDV$ and the explanatory variables are the remaining 13 variables shown in the list. As we can see when loading the data, there are more variables than the ones listed ($TOWN$, $TOWNNO$, $LON$, $LAT$, $CMEDV$). We decided not to use them since the statement explicitly defines which to use.

```{r, echo=TRUE}
load("boston.Rdata")
names(boston.c)
dataset <- boston.c[,-c(1:5)] #"TOWN"    "TOWNNO"  "TRACT"   "LON"     "LAT"
dataset$CMEDV <- NULL
```

Beside from eliminating variables, we divided the dataset between train and test. To do so, we have decided to do 3/4 training, 1/4 test.
```{r, echo=TRUE, results = 'hide'}
set.seed(42)
trainIndex <- createDataPartition(dataset$MEDV, p = 0.75, list = F)
train <- dataset[trainIndex,]
test <- dataset[-trainIndex,]
```

```{r}
train
x <- model.matrix(MEDV~., train)[,-1]
y = train$MEDV
y.test = y[-trainIndex]

#perform cross-validation to choose tuning parameter lambda
cv.out <- cv.glmnet(x, y, alpha = 0)
plot(cv.out)
bestlambda <- cv.out$lambda.min #lambda that results in lowest cross validation error
grid <- 10^seq(10,-2,length = 100) #from 10^10 to 10^-2 
ridge.mod <- glmnet(x,y,alpha= 0, lambda = grid, thresh = 1e-12)
#make predictions for lambda = bestlambda 
ridge.pred <- predict(ridge.mod, s=bestlambda, newx = x[-trainIndex,])
```


```{r, echo=TRUE}
library(plotmo)

glmcoef<-coef(ridge.mod,bestlambda )
coef.increase<-dimnames(glmcoef[glmcoef[,1]>0,0])[[1]]
coef.decrease<-dimnames(glmcoef[glmcoef[,1]<0,0])[[1]]

#get ordered list of variables as they appear at smallest lambda
allnames<-names(coef(ridge.mod)[,
            ncol(coef(ridge.mod))][order(coef(ridge.mod)[,
            ncol(coef(ridge.mod))],decreasing=TRUE)])

#remove intercept
allnames<-setdiff(allnames,allnames[grep("Intercept",allnames)])

#assign colors
cols<-rep("gray",length(allnames))
cols[allnames %in% coef.increase]<-"green"      # higher medv is good
cols[allnames %in% coef.decrease]<-"red"        # lower medv is not

plot_glmnet(ridge.mod,label=TRUE,s=bestlambda,col=cols)

```


To select the best model, we now use 10x10-CV using the lambda that best minimised 
the error in cross-validation, which is lambda.ridge.


```{r, echo=TRUE, message=FALSE, warning=FALSE, result='hide'}
trainContrl <- trainControl (method="repeatedcv", number=10, repeats=10)
model <- train(MEDV ~ ., data = train, trControl=trainContrl, method='glmnet',tuneGrid=expand.grid(alpha=0,lambda=bestlambda))
model.ridge.FINAL <- glmnet(x, y, alpha = 0, lambda = bestlambda)



normalized <- (length(train$MEDV)-1)*var(train$MEDV)
NMSE.ridge.train.error <- crossprod(predict (model) - train$MEDV) / normalized
normTest <- (length(test$MEDV)-1)*var(test$MEDV)
sse_raw <- test$MEDV - model.ridge.FINAL$a0- data.matrix(test[,-dim(train)[2]]) %*% model.ridge.FINAL$beta 
sse <- crossprod (as.matrix(sse_raw)) 

NMSE.ridge.test.error <- sse/normTest
```




```{r,  echo = FALSE, message=FALSE, warning=FALSE,results='asis'}
error.models <- tibble(regression_method = "ridge regression",
                        train_MSE = as.numeric(NMSE.ridge.train.error),
                        test_MSE = as.numeric(NMSE.ridge.test.error))
error.models%>%
  kable(format = "latex", booktabs = TRUE , digits = 3,
        caption = "Model Errors Summary") %>% 
  kable_styling(position = "center",latex_options = c("HOLD_position"),
                font_size = 9)
```