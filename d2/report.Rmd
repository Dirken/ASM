---
title: 'Advanced Statistical Modelling: Ridge Regression'
author: "Ricard Meyerhofer & Joel Cantero"
date: "29/10/2019, correction 20/11/2019"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("ggplot2")
library("car")
library("caret")
library("dplyr")
library("GGally")
library("kableExtra")
library("glmnet")
library("plotmo")
library("data.table")
set.seed(42)
```

# Choosing the penalization parameter $\lambda$
The objective of this exercise is to implement Ridge Regression with two different approaches: $MSPE_{val}(\lambda)$ and $MSPE_{k-CV}(\lambda)$. In both cases we are going to take as input data the following:
\begin{itemize}
\item Matrix x and vector y corresponding to the training sample.
\item Matrix $x_{val}$ and vector $y_{val}$ corresponding to the validation set.
\item Vector $lambda.v$ of candidate values for $\lambda$.
\end{itemize}
We are going to output for each element $\lambda$ in $lambda.v$ and the value of the $MSPE_{val}(\lambda)$ / $MSPE_{k-CV}(\lambda)$. Furthermore, we are going to plot these values against $log(1+ \lambda)-1$.

Once we have build these two functions, we are going to use the prostate data used in class. We are going to choose a $\lambda$ according to:
\begin{itemize}
\item Behaviour in the validation set (30 validations not included in the training sample)
\item 5-fold, 10-fold cross-validation.
\item Compare our results with those obtained with leave-one-out and generalized cross-validation.
\end{itemize}

## Ridge regression based on $MSPE_{val}(\lambda)$ 
In order to choose the penalization parameter $\lambda$, we are going to write a function implementing the ridge regression penalization parameter $\lambda$ choice based on the minimization of the $MSPE_{val}(\lambda)$.

```{r lambda_function}
MSPEval <- function(X, Y, Xval, Yval, lambda.v, n.lambdas) {
  PMSE.VAL <- n.lambdas
  df <- rep(0,length(n.lambdas))
  p <- dim(X)[2]
  # Iterate through candidate values
  x.svd <- svd(x)
  for (l in 1:n.lambdas){
    lambda <- lambda.v[l]
    r <- dim(Xval)[1]
    PMSE.VAL[l] <- 0
    # Compute beta with the traning dataset
    beta.i <- solve(t(X)%*%X + lambda*diag(1,p)) %*% t(X) %*% Y
    for (i in 1:r){
      # Compute the errors with the validation dataset
      Xi <- Xval[i,]
      Yi <- Yval[i]
      y.hat <- Xi %*% beta.i
      PMSE.VAL[l] <-PMSE.VAL[l] + (y.hat - Yi)^2
      df <- sum(x.svd$d^2 / (x.svd$d^2 +lambda))
    }
    PMSE.VAL[l] <- PMSE.VAL[l]/r
  }
  return(PMSE.VAL)
}
```

```{r, include=FALSE}
prostate <- fread(file="prostate_data.txt", header=TRUE)
prostate$train <- as.logical(prostate$train)
prostate$No <- NULL
prostate$train <- as.logical(prostate$train)
prostate <- as.data.frame(prostate)
data <- prostate[,1:9]
lambda.max <- 1e5
n.lambdas <- 20
lambda.v <- exp(seq(0,log(lambda.max+1),length=n.lambdas))-1
validation.ind <- prostate$train
validation <- data[!validation.ind,]
training <- data[validation.ind,]

x <- scale(training[,1:8], center = T, scale = T)
y <- scale(training[,9], center = T, scale = F)

x.val <- scale(validation[,1:8], center = T, scale = T)
y.val <- scale(validation[,9], center = T, scale = F)

p <- ncol(x)
n <- dim(x)[1]
#here
beta.p <- matrix(0,nrow=n.lambdas, ncol=p)
diag.H <- matrix(0,nrow=n.lambdas, ncol=n)
```

### Penalization parameter 

```{r, echo=FALSE, fig.height=6, fig.width=12, message=FALSE, warning=FALSE}
MSPE.val <- MSPEval(x, y, x.val, y.val, lambda.v, n.lambdas)
lambda.val <- lambda.v[which.min(MSPE.val)]
plot(log(1+lambda.v), MSPE.val)
abline(v=log(1+lambda.val),col=2,lty=2)
```

## Ridge regression based on $MSPE_{k-CV}(\lambda)$
Now, we will write an R function implementing the ridge regression penalization parameter$\lambda$ choice based on k-fold cross-validation $MSPE_{kâˆ’CV}(\lambda)$. 

```{r}
MSPEkfold <- function(X, Y, K, n.lambdas, lambda.v, n) {
  n <- dim(X)[1]
  folds <- sample(rep(1:K, length=n), n, replace=FALSE) 
  MPSE <- data.frame()
  # Iterate through K Folds
  for (k in 1:K){
    Xk <- as.matrix(X[folds != k,])
    Yk <- as.matrix(Y[folds != k])
    Xv <- as.matrix(X[folds == k,])
    Yv <- as.matrix(Y[folds == k])
    mspe.k.val = MSPEvalidation(Xk, Yk, Xv, Yv, lambda.v, n.lambdas)
    MPSE <- rbind(MPSE, mspe.k.val) 
  }
  MPSE2 <- c()
  for (i in 1:n.lambdas){
    MPSE2 <- append(MPSE2, mean(MPSE[,i]))
  }
  return(MPSE2)
}

MSPEvalidation <- function(X, Y, Xval, Yval, lambda.v, n.lambdas) {
  PMSE.VAL <- n.lambdas
  p <- ncol(X)
  # Iterate through candidate values
  for (l in 1:n.lambdas){
    lambda <- lambda.v[l]
    sizeXval <- dim(Xval)[1]
    PMSE.VAL[l] <- 0
    # Compute beta with the traning dataset
    beta.i <- solve(t(X)%*%X + lambda*diag(1,p)) %*% t(X) %*% Y
    for (i in 1:sizeXval){
      # Compute the errors with the validation dataset
      Xi <- Xval[i,]; Yi <- Yval[i]
      y.hat <- Xi %*% beta.i
      PMSE.VAL[l] <-PMSE.VAL[l] + (y.hat - Yi)^2
    }
    PMSE.VAL[l] <- PMSE.VAL[l]/sizeXval
  }
  return(PMSE.VAL)
}

```

### Penalization parameter 5-fold and 10-fold.

```{r, echo=FALSE, fig.height=6, fig.width=12}
mspe.5 <- MSPEkfold(x, y, 5, n.lambdas, lambda.v, n)
lambda.mspe.5 <- lambda.v[which.min(mspe.5)]
op<-par(mfrow=c(1,2))
plot(log(1+lambda.v), mspe.5)
abline(v=log(1+lambda.mspe.5),col=2,lty=2)


mspe.10 <- MSPEkfold(x, y, 10, n.lambdas, lambda.v, n)
lambda.mspe.10 <- lambda.v[which.min(mspe.10)]
plot(log(1+lambda.v), mspe.10)
abline(v=log(1+lambda.mspe.10),col=2,lty=2)
```

## Comparation between penalization parameters of $MSPE_{val}(\lambda)$ and $MSPE_{k-CV}(\lambda)$ with other models

```{r prostate}
MSPEcv <- function(X, Y, n.lambdas, lambda.v, n) {
  PMSE.CV <- numeric(n.lambdas)
  p <- ncol(X)
  for (l in 1:n.lambdas){
    lambda <- lambda.v[l]
    PMSE.CV[l] <- 0
    for (i in 1:n){
      X.i <- X[-i,]
      Y.i <- Y[-i]
      Xi <- X[i,]
      Yi <- Y[i]
      beta.i <- solve(t(X.i)%*%X.i + lambda*diag(1,p)) %*% t(X.i) %*% Y.i
      y.hat <- Xi %*% beta.i
      PMSE.CV[l] <- PMSE.CV[l] + (y.hat-Yi)^2
    }
    PMSE.CV[l] <- PMSE.CV[l]/n
  }
  return(PMSE.CV)
}

#generalized cv
MSPEgcv <- function(X, Y, n.lambdas, lambda.v, beta.path, diag.H.lambda, n) {
    PMSE.GCV <- numeric(n.lambdas)
    for (l in 1:n.lambdas){
        hat.Y <- X %*% beta.path[l,]
        nu <- sum(diag.H.lambda[l,])
        PMSE.GCV[l] <- sum( ((Y-hat.Y)/(1-nu/n))^2 )/n
    }
    return(PMSE.GCV)
}
```

```{r, echo=FALSE, fig.height=6, fig.width=12, message=FALSE, warning=FALSE}
mspe.cv <- MSPEcv(x, y, n.lambdas, lambda.v, n)
lambda.cv<- lambda.v[which.min(mspe.cv)]
op<-par(mfrow=c(1,2))
plot(log(1+lambda.v), mspe.cv)
abline(v=log(1+lambda.cv),col=2,lty=2)

mspe.gcv <- MSPEgcv(x, y, n.lambdas, lambda.v, beta.p, diag.H, n)
lambda.gcv<- lambda.v[which.min(mspe.gcv)]
plot(log(1+lambda.v), mspe.gcv)
abline(v=log(1+lambda.gcv),col=2,lty=2)

```


We can see that the Validation approach has the lowest optimal number of degrees of freedom. Furtheremore,  we can find the degrees of freedom of each method and their MSPE. As we can see, LOO method has same degrees of freedom than 10 fold and 5-10 fold also very similar MSPE than validation.


```{r, echo=FALSE, fig.height=6, fig.width=12, message=FALSE, warning=FALSE}
df.v <- numeric(n.lambdas)
XtX <- t(x)%*%x
d2 <- eigen(XtX,symmetric = TRUE, only.values = TRUE)$values
for (l in 1:n.lambdas){
    lambda <- lambda.v[l]
    df.v[l] <- sum(d2/(d2+lambda)) 
}

df <- data.table("MSPE Method" = character(), "df.v" = numeric(), "MSPE" = numeric(),
    stringsAsFactors = FALSE)

df <- rbind(df, list("Validation", df.v[which.min(MSPE.val)], MSPE.val[which.min(MSPE.val)]))
df <- rbind(df, list("Leave-one-out (CV)", df.v[which.min(mspe.cv)], mspe.cv[which.min(mspe.cv)]))
df <- rbind(df, list("5-Fold val.", df.v[which.min(mspe.5)], mspe.5[which.min(mspe.5)]))
df <- rbind(df, list("10-Fold val.", df.v[which.min(mspe.10)], mspe.10[which.min(mspe.10)]))
df <- rbind(df, list("Generalized C.V.", df.v[which.min(mspe.gcv)], mspe.gcv[which.min(mspe.gcv)]))

kable(df)
```

# Ridge regression for the Boston Housing data

The Boston Housing dataset is a classical dataset which containes the values of 506 suburbs of Boston corresponding to 1978. This dataset can be found in many places but we are going to use a version with some corrections that was provided to us, which additionally includes the UTM coordinates of the geographical centers of each neighborhood. Therefore, the variables are the following:

Variable  | Description                           | Type
:--------------:|:-----------------------------------------------------------------------------------:|:--------------:
CRIM | per capita crime rate by town |  Numeric |
ZN | proportion of residential land zoned for lots over 25,000 sq.ft. |  Numeric|
INDUS | proportion of non-retail business acres per town |  Numeric|
CHAS | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) | Factor |
NOX | nitric oxides concentration (parts per 10 million) |  Numeric|
RM | average number of rooms per dwelling |  Numeric |
AGE | proportion of owner-occupied units built prior to 1940 | Numeric |
DIS | weighted distances to five Boston employment centres | Numeric |
RAD | index of accessibility to radial highways | Numeric |
TAX | full-value property-tax rate per $10,000 |  Numeric |
PTRATIO | pupil-teacher ratio by town |  Numeric |
B | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town | Numeric  |
LSTAT | % lower status of the population | Numeric  |
MEDV | Median value of owner-occupied homes in $1000's | Numeric |


In this exercise we are going to use ridge regression on the Boston Housing dataset to fit the regression model where the response variable is $MEDV$ and the explanatory variables are the remaining 13 variables shown in the list. As we can see when loading the data, there are more variables than the ones listed ($TOWN$, $TOWNNO$, $LON$, $LAT$, $CMEDV$). We decided not to use them since the statement explicitly defines which to use.

```{r, echo=TRUE, fig.height=4, fig.width=12, message=FALSE, warning=FALSE, results = 'hide'}
load("boston.Rdata")
names(boston.c)
dataset <- boston.c[,-c(1:5)] #"TOWN"    "TOWNNO"  "TRACT"   "LON"     "LAT"
dataset$CMEDV <- NULL
```

Besides from eliminating variables, we divided the dataset between train and test. To do so, we have decided to do 3/4 training, 1/4 test.
```{r, echo=TRUE, fig.height=4, fig.width=12, message=FALSE, warning=FALSE, results = 'hide'}
trainIndex <- createDataPartition(dataset$MEDV, p =0.75, list = F)
train <- dataset[trainIndex,]
test <- dataset[-trainIndex,]



```



We are going to compare the different models and choose lambda with the one that performs best.
```{r, fig.height=4, fig.width=12, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
x <- model.matrix(MEDV~., train)[,-1]
y <- train$MEDV
Xval <- model.matrix(MEDV~., test)[,-1]
Yval <- test$MEDV
#perform cross-validation to choose tuning parameter lambda
lambda.max <- 1e5  
n.lambdas <- 50
lambda.v <- exp(seq(10^-2,log(lambda.max+1),length=n.lambdas))-1  
n <- dim(x)[1]
p <- dim(x)[2]
XtX <- t(x)%*%x
d2 <- eigen(XtX,symmetric = TRUE, only.values = TRUE)$values
df.v <- numeric(n.lambdas)
for (l in 1:n.lambdas){
  lambda <- lambda.v[l]
  df.v[l] <- sum(d2/(d2+lambda)) 
}
beta.path <- matrix(0,nrow=n.lambdas, ncol=p)
diag.H.lambda <- matrix(0,nrow=n.lambdas, ncol=n)
for (l in 1:n.lambdas){ 
  lambda <- lambda.v[l]
  H.lambda.aux <- t(solve(XtX + lambda*diag(1,p))) %*% t(x) 
  beta.path[l,] <-  H.lambda.aux %*% y
  H.lambda <- x %*% H.lambda.aux 
  diag.H.lambda[l,] <- diag(H.lambda)
} 
```

```{r, echo=FALSE, fig.height=6, fig.width=12, fig.align='center'}
## MSPE Val
MSPE.val <- MSPEval(x, y, Xval, Yval, lambda.v, n.lambdas)
lambda.val <- lambda.v[which.min(MSPE.val)]

## MSPE K-Fold validation
mspe.10 <- MSPEkfold(x, y, 10, n.lambdas, lambda.v, n)
mspe.5 <- MSPEkfold(x, y, 5, n.lambdas, lambda.v, n)

## MSPE Leave one out validation
mspe.cv <- MSPEcv(x, y, n.lambdas, lambda.v, n)
lambda.CV <- lambda.v[which.min(mspe.cv)]

## MSPE GCV
mspe.gcv <- MSPEgcv(x, y, n.lambdas, lambda.v, beta.path, diag.H.lambda, n)
df.GCV <- df.v[which.min(mspe.gcv)]

## Plotting
plot(df.v, MSPE.val)
points(df.v, mspe.cv,col=13,pch=19,cex=.75)
points(df.v, mspe.5,col=12,pch=19,cex=.75)
points(df.v, mspe.10,col=8,pch=19,cex=.75)
points(df.v, mspe.gcv,col=10,pch=2,cex=1.5)

abline(v=df.GCV,col=1,lty=2,lwd=2)
abline(v=0.001,col=2,lty=2,lwd=1, cex=0.35) # Since df.CV.H.lambda == 0, this is to force the print of this abline
# abline(v=df.CV.H.lambda + 0.001,col=2,lty=2, lwd=3)

legend("top",
       c("MSPE.val","MSPE.CV", "MSPE 5-Fold", "MSPE 10-Fold", "MSPE GCV", "lambda.GCV","lambda.CV"),
       pch=c(2, 19, 19, 19, 2, NA, NA),
       lty=c(0,0,0,0,0,2,2),
       lwd=c(0,0,0,0,0,2,1),
       col=c(9, 13, 12, 8, 10, 1, 2)
       )
```
```{r, echo=FALSE}
df <- data.table("MSPE Method" = character(), "df.v" = numeric(), "MSPE" = numeric(), stringsAsFactors = FALSE)
df <- rbind(df, list("Validation", df.v[which.min(MSPE.val)], MSPE.val[which.min(MSPE.val)]))
df <- rbind(df, list("Leave-one-out (CV)", df.v[which.min(mspe.cv)], mspe.cv[which.min(mspe.cv)]))
df <- rbind(df, list("5-Fold val.", df.v[which.min(mspe.5)], mspe.5[which.min(mspe.5)]))
df <- rbind(df, list("10-Fold val.", df.v[which.min(mspe.10)], mspe.10[which.min(mspe.10)]))
df <- rbind(df, list("Generalized C.V.", df.v[which.min(mspe.gcv)], mspe.gcv[which.min(mspe.gcv)]))

kable(df)
```
We can see from both the graphic and the table that the lowest values have been obtained again with the Validation method, with a selected degrees of freedom of 10.90 and a MSPE of 24.94.

The generalized cross validation method as well as the leave one out are very close to the K-Fold methods and their evolution over different is very similar.
 

## Choosing $\lambda$ 

Now that we know which is the method that performs the best, we are going to see which $\lambda$ is the best
```{r, fig.height=4, fig.width=12, message=FALSE, warning=FALSE}
plot(log(1+lambda.v), MSPE.val)
abline(v=log(1+lambda.val),col=2,lty=2)
```
```{r, include=FALSE}
(minPos = which.min(MSPE.val))
lambda.v[minPos]
```
We can see that the 19th lambda is the one with the lowest MSPE value with $\lambda_{19} = 68.10$.

